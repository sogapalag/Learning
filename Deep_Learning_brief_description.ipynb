{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Binary Classification: $x:R^n \\rightarrow y:\\{0,1\\}$\n",
    "\n",
    "Recall Logit.R. $\\hat{y}=\\sigma(w^T x+b)$. Loss func $l=-y\\log \\hat{y} -(1-y)\\log(1-\\hat{y})$. Cost func, $J(w,b)=$ average $l$.\n",
    "\n",
    "Gradient descent\n",
    "\n",
    "----------------------\n",
    "\n",
    "#### Activation funcs\n",
    "\n",
    "sigmod, tanh, ReLU, leaky ReLU. 导数形式简单\n",
    "\n",
    "---------------------\n",
    "\n",
    "Backpropagation(BP), 反向计算梯度更新\n",
    "\n",
    "Layer, $z=Wx+b$, => activation $g(z)$.\n",
    "\n",
    "dropout, 一个trick, 即以概率p放弃采纳该元的参数值，使得整体不会过于依赖某些feature，避免overfitting. (regularization)\n",
    "\n",
    "-------------------\n",
    "\n",
    "## NLP\n",
    "\n",
    "\n",
    "Distributional semantics: A word’s meaning is given by the words that frequently appear close-by.(modern statistical NLP!)\n",
    "\n",
    "word vectors(also called word embeddings/word representations), is distributed representation.\n",
    "\n",
    "#### Word2Vec\n",
    "\n",
    "a word, center/outside vector.\n",
    "\n",
    "prediction function: $P(o|c) = \\frac{\\exp(u_o^T v_c)}{\\sum_{w\\in V}\\exp(u_w^T v_c)}$. 点积蕴涵相似性，式子形式即softmax。\n",
    "\n",
    "objective func, $J=\\frac{1}{T}\\sum_t^T \\sum_{-m<j\\neq 0<m} \\log P(w_{t+j}|w_t)$ MLE.\n",
    "\n",
    "Stochastic gradient descent (SGD), 解决gradient难(expensive)计算的问题。\n",
    "\n",
    "###### word2vec variants\n",
    "\n",
    "continuous bag-of-words(CBOW): 即基于context(左右window-size words) predict center word. measure(cross-entropy).\n",
    "\n",
    "skip-gram: 即基于center word算context；（作了独立假设，window words与距离独立）。negative-sampling, a approx trick.\n",
    "\n",
    "###### examples of tasks\n",
    "\n",
    "Easy: Spell Checking; Keyword Search;Finding Synonyms.\n",
    "\n",
    "Midium: Parsing information from websites, documents, etc.\n",
    "\n",
    "Hard: Machine Translation; Semantic Analysis (What is the meaning of query statement?); Coreference (e.g. What does \"he\" or \"it\" refer to given a document?); Question Answering (e.g. Answering Jeopardy questions).\n",
    "\n",
    "-----------------------\n",
    "\n",
    "#### [GloVe](https://nlp.stanford.edu/pubs/glove.pdf)\n",
    "\n",
    "Co-occurrence Matrix: 即统计word i附近(window-size)出现word j的次数。\n",
    "\n",
    "由于一系列efficiency需要，化约为$J=\\sum_i^W \\sum_j^W f(X_{ij}) (u_j^T v_i - \\log X_{ij})^2$. 或直接根据一些假设推导出带bias term的等价形式。\n",
    "\n",
    "GloVe >(better,faster) Word2Vec on word analogy task. (由于只统计非0，且重要的global相关性统计直接利用进来了)\n",
    "\n",
    "Evaluation (TODO)\n",
    "\n",
    "----------------------\n",
    "\n",
    "Named Entity Recognition (NER)：识别词语属性e.g. 国家，人名，组织，etc\n",
    "\n",
    "Matrix表示，NN，Jacob.\n",
    "\n",
    "-------------------------\n",
    "\n",
    "基本上always用已训练的word vectors；\"fine tune\"也基本不用，除非本地数据很大。\n",
    "\n",
    "[Back-prop](https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b)(反向传播)，即算偏导\n",
    "\n",
    "(leaky)ReLU, 在deep里表现强劲（一些其它[技巧](http://web.stanford.edu/class/cs224n/slides/cs224n-2019-lecture04-backprop.pdf)）\n",
    "\n",
    "[梯度优化算法小结](http://ruder.io/optimizing-gradient-descent/)\n",
    "\n",
    "Adam: m,v本质上算dJ,dJ^2的期望估计，然后以$-\\frac{\\alpha}{\\sqrt{v}} m $作descent.\n",
    "\n",
    "RMSProp: 有点与Adam类似，以$-\\frac{\\alpha}{\\sqrt{v}} dJ$ 作descent.\n",
    "\n",
    "[dropout](https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf)\n",
    "\n",
    "------------------------\n",
    "\n",
    "Dependency Parsing (TODO)\n",
    "\n",
    "-------------------------\n",
    "\n",
    "[Language Model](http://web.stanford.edu/class/cs224n/slides/cs224n-2019-lecture06-rnnlm.pdf): 其它NLP的一个子任务基础。预测接下来的词，即$P(x^{(t+1)}|x^{(t)},...,x^{(1)})$. e.g. 搜索输入预测。当下强大的RNN-LM，还可以用来generate style-text.\n",
    "\n",
    "#### RNNs\n",
    "\n",
    "\\begin{align}\n",
    "    \\frac{\\partial J^{(t)}}{\\partial W_h} = \\sum_{i=1}^t \\frac{\\partial J^{(t)}}{\\partial W_h}|_{(i)}\n",
    "\\end{align}\n",
    "\n",
    "Evaluating, perplexity $=\\exp(J(\\theta))$.\n",
    "\n",
    "vanish gradient problem -> GRU/LSTM. ([fancy RNNs](http://web.stanford.edu/class/cs224n/slides/cs224n-2019-lecture07-fancy-rnn.pdf))\n",
    "\n",
    "即由于RNN用一样的W, 根据反向传播gradient会exponentially传递，牵涉到long-distance-dependencies的问题.\n",
    "Gradient clipping (for explloding), 即在update前，scale down.\n",
    "\n",
    "#### LSTM/GRU\n",
    "\n",
    "[LSTM](https://colah.github.io/posts/2015-08-Understanding-LSTMs/), 增加一个cell存长期信息，需要额外的forget/input/output gate. Successful tasks include: handwriting recognition, speech recognition, machine translation, parsing, image captioning\n",
    "\n",
    "GRU, no cell(可以看作合并到hidden state里了), 更少参数。看fancy RNNs里 GRU vs LSTM.\n",
    "\n",
    "Bidir..RNN，双向; Multi-layer RNNs\n",
    "\n",
    "a summary: 1.LSTMs are powerful but GRUs are faster; 2. Clip your gradients; 3. Use bidirectionality when possible; 4. Multi-layer RNNs are powerful, but you might need skip/dense-connections if it’s deep.\n",
    "\n",
    "-------------------\n",
    "\n",
    "(Neural) Machine Translation\n",
    "\n",
    "seq2seq: contional LM, encoder->作为decoder input, LM预测contion on 源句。 Beam（为efficiency 以5-10个词评score）。with attention，增加一个源词组分布权重；decoder hidden state可以以此权重query到所有encoder的hidden states值\n",
    "\n",
    "-------------------\n",
    "\n",
    "webs: [ACL](https://aclanthology.info/), [smt](http://statmt.org/), [dep-parsing](https://universaldependencies.org/), [dataset1](https://github.com/niderhoff/nlp-datasets), [dataset2](https://machinelearningmastery.com/datasets-natural-language-processing/). [arxiv-sanity](http://www.arxiv-sanity.com/) , [state-of-art](https://paperswithcode.com/sota)\n",
    "\n",
    "[trainning strategy](http://web.stanford.edu/class/cs224n/slides/cs224n-2019-lecture09-final-projects.pdf)\n",
    "\n",
    "BERT (TODO)\n",
    "\n",
    "Transformers (TODO)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tips\n",
    "\n",
    "LSTM as default, 特别数据有很强的依赖，或数据很大时。要快一些可以switch到GRU。（用的最多的俩gated-RNN）\n",
    "\n",
    "clip gradients\n",
    "\n",
    "如果可以用双向，（右context可获取时）\n",
    "\n",
    "多层RNN很强，不过需要skip/dens-连接\n",
    "\n",
    "除了MT, seq2seq还可以做Summarization，Dialogue，Parsing，Code generation这种'译'任务\n",
    "\n",
    "---------------\n",
    "\n",
    "#### train gated-RNN\n",
    "\n",
    "用LSTM 或 GRU\n",
    "\n",
    "Initialize recurrent matrices to be orthogonal\n",
    "\n",
    "Initialize other matrices with a sensible (small!) scale\n",
    "\n",
    "Initialize forget gate bias to 1: default to remembering\n",
    "\n",
    "Use adaptive learning rate algorithms: Adam, AdaDelta, …\n",
    "\n",
    "Clip the norm of the gradient: 1–5 seems to be a reasonable. threshold when used together with Adam or AdaDelta.\n",
    "\n",
    "Either only dropout vertically or look into using Bayesian. ropout (Gal and Gahramani – not natively in PyTorch)\n",
    "\n",
    "Be patient! Optimization takes time\n",
    "\n",
    "#### Experimental strategy\n",
    "\n",
    "从简单模型开始，一点一点添加\n",
    "\n",
    "从小数据开始，应该有100%拟合\n",
    "\n",
    "在大数据，train也应该有接近100%的拟合\n",
    "\n",
    "L2-正则化，or 基本上dropout很有用。\n",
    "\n",
    "要表现很好，tune hyperparam.很重要\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
